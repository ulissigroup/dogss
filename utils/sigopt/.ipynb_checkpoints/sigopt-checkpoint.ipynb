{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document demonstrates the making, training, saving, loading, and usage of a sklearn-compliant CGCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0,'/home/junwoony/.local/lib/python3.6/site-packages')\n",
    "# sys.path.insert(0,'../')\n",
    "import numpy as np\n",
    "#Select which GPU to use if necessary\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "import mongo\n",
    "# from torchviz import make_dot, make_dot_from_trace\n",
    "import time\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mongo\n",
    "# from cgcnn.data_icgcnn import StructureData, ListDataset, StructureDataTransformer\n",
    "from cgcnn.data_grad_surface import StructureData, ListDataset, StructureDataTransformer\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "\n",
    "import multiprocess as mp\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset as mongo docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle.load(open('../input/intermetallics_cleavage_energy_data.pkl', 'rb'))\n",
    "\n",
    "# random.seed(123)\n",
    "# random.shuffle(docs)\n",
    "for doc in docs:\n",
    "    doc[\"atoms\"] = doc['thinnest_structure']['atoms']\n",
    "    doc[\"results\"] = doc['thinnest_structure']['results']\n",
    "    doc[\"initial_configuration\"] = doc['thinnest_structure']['initial_configuration']\n",
    "    del doc[\"thinnest_structure\"]\n",
    "    \n",
    "SDT_list = pickle.load(open('../input/SDT_surface_new.pkl', 'rb'))\n",
    "structures = SDT_list[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "target_list = np.array([sdt[-1].numpy() for sdt in SDT_list]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the features from the data transformer, to be used in setting up the net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "determine max connectivity value (for radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import mongo\n",
    "# from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "# import numpy as np\n",
    "# import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# SDT = StructureDataTransformer(atom_init_loc='/home/zulissi/software/cgcnn_sklearn/atom_init.json',\n",
    "#                               max_num_nbr=12,\n",
    "#                               step=0.2,\n",
    "#                               radius=1,\n",
    "#                               use_tag=False,\n",
    "#                               use_fixed_info=False)\n",
    "\n",
    "# SDT_out = SDT.transform(docs)\n",
    "\n",
    "# structures = SDT_out[0]\n",
    "# orig_atom_fea_len = structures[0].shape[-1]\n",
    "# nbr_fea_len = structures[1].shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 19\n"
     ]
    }
   ],
   "source": [
    "print(orig_atom_fea_len, nbr_fea_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGCNN model with skorch to make it sklearn compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "\n",
    "from cgcnn.data_grad_surface import collate_pool, MergeDataset\n",
    "from cgcnn.model_grad_2_surface_simple_sigopt import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch\n",
    "import skorch.callbacks.base\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example converting all the documents up front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Make the target list\n",
    "# import seaborn as sns\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# target_list = np.array(energies).reshape(-1,1)\n",
    "# sns.distplot(target_list, color='black')\n",
    "    \n",
    "# #scaler = StandardScaler().fit(energies.reshape(-1, 1))\n",
    "# scaler = MinMaxScaler().fit(energies.reshape(-1, 1))\n",
    "# target_list = scaler.transform(energies.reshape(-1,1))\n",
    "# print(type(target_list))\n",
    "\n",
    "# sns.distplot(target_list, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversed_target_list = scaler.inverse_transform(target_list)\n",
    "# sns.distplot(inversed_target_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SDT_list= SDT_list\n",
    "target_list = target_list\n",
    "\n",
    "indices = np.arange(len(SDT_list))\n",
    "SDT_training, SDT_test, target_training, target_test, train_idx, test_idx \\\n",
    "= train_test_split(SDT_list, target_list, indices, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamwr.adamw import AdamW\n",
    "from torch.optim.lbfgs import LBFGS\n",
    "\n",
    "from adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "\n",
    "# batchsize = (10,300)\n",
    "# # warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "# LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=batchsize, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "LR_schedule = LRScheduler('MultiStepLR',milestones=[100],gamma=0.1)\n",
    "\n",
    "#############\n",
    "# To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):        \n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        differ=torch.sum((y_pred-y_true.cuda())**2.0,dim=1)\n",
    "        if torch.nonzero(differ).shape[0] != differ.shape[0]:\n",
    "            print('zero sqrt for Loss')\n",
    "#             zero_idx = (differ == 0).nonzero()\n",
    "#             differ[zero_idx] = 1e-6\n",
    "        differ = torch.clamp(differ, min=1e-8)\n",
    "        return torch.mean(torch.sqrt(differ))\n",
    "#         return torch.mean(torch.sqrt(torch.sum((y_pred-y_true.cuda())**2.0,dim=1)))\n",
    "#         return super().get_loss(y_pred, y_true, **kwargs)\n",
    "## return features = net.forward(SDT_test)\n",
    "\n",
    "\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "#     module__angle_fea_len = 8, #angle_fea_len,\n",
    "    batch_size=(10,300), #214\n",
    "    module__classification=False,\n",
    "    lr=(np.exp(-15),np.exp(-3)),\n",
    "    max_epochs= (50, 800),\n",
    "    module__atom_fea_len=(4,256), #46,\n",
    "    module__h_fea_len=(32, 256),\n",
    "    module__n_conv=(1,10), #8\n",
    "    module__n_h=(1,10),\n",
    "    module__max_num_nbr=12, #9\n",
    "    module__opt_step_size=(0.01, 0.9), #0.3\n",
    "    module__min_opt_steps=30,\n",
    "    module__max_opt_steps=300,\n",
    "    module__momentum=(0.1,0.9),\n",
    "    module__dropout=(0, 0.3),\n",
    "    module__dropout_h=(0, 0.3),\n",
    "    optimizer__weight_decay=(1e-6, 1e-2),\n",
    "    optimizer=AdamW,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp, LR_schedule, load_best_valid_loss] #    callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sigopt_sklearn.search import SigOptSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "client_token = \"TSRIPFKLRAIMUDDVQEBJHVBQRVBCDJOSKJMKEQTXWCYZDNED\"\n",
    "net_parameters = {\n",
    "                'batch_size':(10,300),\n",
    "                'lr':(np.exp(-15),np.exp(-3)),\n",
    "                'max_epochs': (50, 800),\n",
    "                'module__atom_fea_len':(4,256), #46,\n",
    "                'module__h_fea_len':(32, 256),\n",
    "                'module__n_conv':(1,10), #8\n",
    "                'module__n_h':(1,10),\n",
    "                'module__opt_step_size':(0.01, 0.9), #0.3\n",
    "                'module__momentum':(0, 0.9),\n",
    "                'module__dropout':(0, 0.3),\n",
    "                'module__dropout_h':(0, 0.3),\n",
    "                'optimizer__weight_decay':(1e-6, 1e-2)\n",
    "                }\n",
    "\n",
    "clf = SigOptSearchCV(net, net_parameters, cv=train_test_splitter, client_token=client_token,\n",
    "                    n_jobs=1, n_iter=50, scoring=get_scorer('neg_mean_absolute_error'))\n",
    "\n",
    "len(net_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junwoony/miniconda3/envs/schnet2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.1538\u001b[0m        \u001b[32m0.1525\u001b[0m     +  3.9325\n",
      "      2        0.1574        0.1525        1.8917\n",
      "      3        \u001b[36m0.1508\u001b[0m        0.1525        1.8942\n",
      "      4        0.1509        0.1525        1.8097\n",
      "      5        \u001b[36m0.1434\u001b[0m        0.1525        1.8286\n",
      "      6        0.1506        0.1525        1.8397\n",
      "      7        0.1507        0.1525        1.8429\n",
      "      8        0.1510        0.1525        1.9002\n",
      "      9        0.1559        0.1525        1.8103\n",
      "     10        0.1469        0.1525        1.8236\n",
      "     11        0.1458        0.1525        1.8492\n"
     ]
    }
   ],
   "source": [
    "clf.fit(SDT_training, target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-baba530bc70f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.best_params_, clf.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size\t74\n",
    "lr\t0.010390047081273095\n",
    "max_epochs\t272\n",
    "module__atom_fea_len\t82\n",
    "module__h_fea_len\t120\n",
    "module__n_conv\t4\n",
    "module__n_h\t3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

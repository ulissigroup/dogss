{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skorch/net.py --> evaluation_step --> set training=True\n",
    "# skorch/net.py --> validation --> comment out \"with no_grad()\"\n",
    "\n",
    "# skorch/dataset.py --> get_len(data) -- > comment if(len_data)!=1 out\n",
    "# skorch/callbacks/scoring.py --> comment \"is_multimetrics\" out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import mongo\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from dogss.data import collate_pool, MergeDataset\n",
    "from dogss.dogss import DOGSS\n",
    "\n",
    "import skorch\n",
    "from skorch.dataset import CVSplit\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn.metrics import get_scorer\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "from skorch.callbacks.lr_scheduler import LRScheduler\n",
    "\n",
    "from utils.adamwr.adamw import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_list_path = 'path/to/dataset'\n",
    "docs_path = 'path/to/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "SDT_list = pickle.load(open(SDT_list_path , 'rb'))\n",
    "docs = pickle.load(open(docs_path, 'rb'))\n",
    "target_list = np.array([sdt[-1][sdt[-2]].numpy() for sdt in SDT_list]).reshape(-1,1) #get final_pos of free atoms ONLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = SDT_list[0]\n",
    "orig_node_fea_size = structures[0].shape[-1]\n",
    "edge_fea_size = structures[1].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lbfgs import LBFGS\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn.metrics import get_scorer\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "import skorch.callbacks.base\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDT_training, SDT_test, target_training, target_test, docs_training, docs_test \\\n",
    "= train_test_split(SDT_list, target_list, docs, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_splitter = ShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "\n",
    "batchsize = 18\n",
    "LR_schedule = LRScheduler(\"MultiStepLR\", milestones=[100], gamma=0.1)\n",
    "\n",
    "class NewDOGSS(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):\n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        differ=torch.sum((y_pred-y_true.cuda())**2.0,dim=1)\n",
    "        if torch.nonzero(differ).shape[0] != differ.shape[0]:\n",
    "            print('zero sqrt for Loss')\n",
    "        differ = torch.clamp(differ, min=1e-8)\n",
    "        return torch.mean(torch.sqrt(differ))\n",
    "\n",
    "\n",
    "net = NewDOGSS(\n",
    "    DOGSS,\n",
    "    batch_size=batchsize, #214\n",
    "    module__orig_node_fea_size = orig_node_fea_size,\n",
    "    module__edge_fea_size = edge_fea_size,\n",
    "    lr=0.0037704604911552916,\n",
    "    max_epochs= 200,\n",
    "    module__energy_mode=\"Harmonic\", #[\"Harmonic\", \"Morse\", \"LJ\"], Default = \"Harmonic\"\n",
    "    module__node_fea_size=103, #46,\n",
    "    module__h_fea_len=169,\n",
    "    module__h_fea_len_dist=18,\n",
    "    module__h_fea_len_const=18,\n",
    "    module__h_fea_len_D=18,\n",
    "    module__n_conv=12, #8\n",
    "    module__n_h_dist=16,\n",
    "    module__n_h_const=0,\n",
    "    module__n_h_D= 12,\n",
    "    module__min_opt_steps=30,\n",
    "    module__max_opt_steps=150,\n",
    "    module__momentum=0.8,\n",
    "    optimizer=AdamW,\n",
    "    optimizer__weight_decay=0.000045399929762484854,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp,LR_schedule , load_best_valid_loss],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # To train DOGSS,\n",
    "# net.initialize()\n",
    "# net.fit(SDT_training, target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Loading pre-trained DOGSS\n",
    "net.initialize()\n",
    "net.load_params(f_history = './valid_best_history.json',\n",
    "               f_optimizer = './valid_best_optimizer.pt',\n",
    "               f_params = './valid_best_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:00, 5197.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17640it [00:04, 4174.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length 9960\n",
      "bond_distance: tensor(0.0158) tensor(0.2098) tensor(0.0847)\n",
      "x: tensor(0.1537) tensor(0.8255) tensor(0.1001)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'/home/junwoony/Desktop/DOGSS/')\n",
    "\n",
    "import numpy as np\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "import mongo\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lbfgs import LBFGS\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer, collate_pool, MergeDataset\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, train_test_split\n",
    "from sklearn.metrics import get_scorer\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint, LoadInitState #needs skorch 0.4.0, conda-forge version at 0.3.0 doesn't cut it\n",
    "import skorch.callbacks.base\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "\n",
    "from utils.adamwr.adamw import AdamW\n",
    "from utils.adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "\n",
    "from sigopt import Connection\n",
    "from sigopt_sklearn.search import SigOptSearchCV\n",
    "\n",
    "\n",
    "SDT_list = pickle.load(open('../../inputs/H_ads/SDT_list_new.pkl', 'rb'))\n",
    "docs = pickle.load(open('../../inputs/H_ads/final_docs_new.pkl', 'rb'))\n",
    "\n",
    "\n",
    "structures = SDT_list[0]\n",
    "orig_atom_fea_len = structures[0].shape[-1]\n",
    "nbr_fea_len = structures[1].shape[-1]\n",
    "\n",
    "SDT_list_new = []\n",
    "docs_new = []\n",
    "for i, sdt in enumerate(SDT_list):\n",
    "    atom_pos_final = sdt[-1]\n",
    "    nbr_fea_idx = sdt[2]\n",
    "    nbr_fea_offset = sdt[3]\n",
    "    cells = sdt[7]\n",
    "    ads_idx_base = sdt[-4]\n",
    "    ads_idx = np.where(ads_idx_base == 1)[0]\n",
    "\n",
    "    nbr_pos = atom_pos_final[nbr_fea_idx]\n",
    "    differ = nbr_pos - atom_pos_final.unsqueeze(1) + torch.bmm(nbr_fea_offset, cells)\n",
    "    differ_sum = torch.sum(differ**2, dim=2)\n",
    "    distance = torch.sqrt(differ_sum).unsqueeze(-1)\n",
    "    \n",
    "    if np.min(distance.numpy()) == 0:\n",
    "        print('filtered idx', i)\n",
    "    else:\n",
    "        SDT_list_new.append(sdt)\n",
    "        docs_new.append(docs[i])\n",
    "        \n",
    "        \n",
    "target_list = np.array([sdt[-1][sdt[-2]].numpy() for sdt in SDT_list]).reshape(-1,1) #get final_pos of free atoms ONLY\n",
    "SDT_list = SDT_list_new\n",
    "docs = docs_new\n",
    "\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "#Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "\n",
    "#Callback to load the checkpoint with the best validation loss at the end of training\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('valid_best_params.pt')\n",
    "        \n",
    "load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "print('device:', device)\n",
    "\n",
    "def get_distance(atom_pos, nbr_fea_idx, nbr_fea_offset, cells):\n",
    "    nbr_pos = atom_pos[nbr_fea_idx]\n",
    "    differ = nbr_pos - atom_pos.unsqueeze(1) + torch.bmm(nbr_fea_offset, cells)\n",
    "    differ_sum = torch.sum(differ**2, dim=2)\n",
    "    distance = torch.sqrt(differ_sum).unsqueeze(-1)            \n",
    "    return distance\n",
    "\n",
    "differences, differences_ads, differences_non_ads_free = [], [], []\n",
    "dist_free, dist_ads, dist_non_ads =[],[], []\n",
    "SDT_list_new, docs_new, target_list_new = [], [], []\n",
    "\n",
    "c = 0\n",
    "max_num_nbr = 12\n",
    "\n",
    "for i, sdt in tqdm.tqdm(enumerate(SDT_list)):\n",
    "    nbr_fea_idx = sdt[2]\n",
    "    nbr_fea_offset = sdt[3]\n",
    "    atom_pos =sdt[4]\n",
    "    cells = sdt[7]\n",
    "    atom_pos_final = sdt[-1]\n",
    "    free_atom_idx = sdt[-2]\n",
    "    ads_idx_base = sdt[-4]\n",
    "    ads_tag = np.where(ads_idx_base == 1)[0]\n",
    "    non_ads_tag = np.sort(np.array(list(set(free_atom_idx.numpy()) ^ set(ads_tag))))\n",
    "\n",
    "#     non_ads_free_base = np.arange(len(atom_pos))\n",
    "#     non_ads_free_atom_idx = free_atom_idx[np.where(free_atom_idx.numpy() != ads_tag)]\n",
    "    \n",
    "    ads_dist = torch.sqrt(torch.sum((atom_pos-atom_pos_final)[ads_tag]**2, dim=1))\n",
    "    non_ads_dist = torch.sqrt(torch.sum((atom_pos-atom_pos_final)[non_ads_tag]**2, dim=1))\n",
    "    \n",
    "    if torch.mean(ads_dist) < 2 and 0.04 < torch.mean(non_ads_dist) < 0.3:\n",
    "        SDT_list_new.append(sdt)\n",
    "        target_list_new.append(target_list[i])\n",
    "        docs_new.append(docs[i])\n",
    "\n",
    "        dist_free.append(torch.sqrt(torch.sum((atom_pos - atom_pos_final)[free_atom_idx]**2, dim=1)))\n",
    "        dist_ads.append(ads_dist)\n",
    "        dist_non_ads.append(non_ads_dist)\n",
    "\n",
    "        bond_distance = get_distance(atom_pos, nbr_fea_idx, nbr_fea_offset, cells)\n",
    "        final_distance = get_distance(atom_pos_final, nbr_fea_idx, nbr_fea_offset, cells)\n",
    "        N, M, C = bond_distance.shape\n",
    "        bond_distance = bond_distance #* fake_nbr.float().expand(N, M, C)    \n",
    "        final_distance = final_distance #* fake_nbr.float().expand(N, M, C) \n",
    "\n",
    "        c += len(atom_pos) * max_num_nbr\n",
    "    #     differences.append((torch.sqrt(torch.tensor(10.))*final_distance - torch.sqrt(torch.tensor(10.))*bond_distance).view(-1))\n",
    "        differences.append((final_distance - bond_distance).view(-1))\n",
    "        differences_ads.append((final_distance - bond_distance).view(-1)[ads_tag])\n",
    "        differences_non_ads_free.append((final_distance - bond_distance).view(-1)[non_ads_tag])\n",
    "\n",
    "differences = torch.cat(differences)**2\n",
    "differences_ads = torch.cat(differences_ads)**2\n",
    "differences_non_ads_free = torch.cat(differences_non_ads_free)**2\n",
    "\n",
    "# differences = torch.clamp(differences, min=1e-8)\n",
    "assert c == len(differences)\n",
    "# dist_err = torch.mean(torch.abs(differences))\n",
    "# torch.log(dist_err)\n",
    "# dist_err = torch.mean(torch.sqrt(differences))\n",
    "dist_err = torch.mean(differences)\n",
    "dist_err_ads = torch.mean(differences_ads)\n",
    "dist_err_non_ads_free = torch.mean(differences_non_ads_free)\n",
    "\n",
    "print('dataset length', len(SDT_list_new))\n",
    "print('bond_distance:',dist_err, dist_err_ads, dist_err_non_ads_free)\n",
    "print('x:',torch.mean(torch.cat(dist_free)), torch.mean(torch.cat(dist_ads)), torch.mean(torch.cat(dist_non_ads)))\n",
    "\n",
    "\n",
    "\n",
    "target_list_new = np.array(target_list_new)\n",
    "\n",
    "SDT_training, SDT_test, target_training, target_test, docs_training, docs_test \\\n",
    "= train_test_split(SDT_list_new, target_list_new, docs_new, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "train_test_splitter = ShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "# train_test_splitter = ShuffleSplit(n_splits=1, test_size=0.1, random_state=22)\n",
    "\n",
    "\n",
    "batchsize = 36\n",
    "# warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "# LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=batchsize, epoch_size=len(SDT_training), restart_period=10, t_mult=1.2)\n",
    "\n",
    "#### For Sigopt\n",
    "LR_schedule = LRScheduler(\"MultiStepLR\", milestones=[100], gamma=0.1)\n",
    "\n",
    "#############\n",
    "# To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "class MyNet(NeuralNetRegressor):\n",
    "    def get_loss(self, y_pred, y_true, **kwargs):\n",
    "        y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "        print(y_pred.shape, y_true.shape)\n",
    "        differ=torch.sum((y_pred-y_true.cuda())**2.0,dim=1)\n",
    "        if torch.nonzero(differ).shape[0] != differ.shape[0]:\n",
    "            print('zero sqrt for Loss')\n",
    "#             zero_idx = (differ == 0).nonzero()\n",
    "#             differ[zero_idx] = 1e-6\n",
    "        differ = torch.clamp(differ, min=1e-8)\n",
    "\n",
    "        return torch.mean(torch.sqrt(differ))\n",
    "\n",
    "\n",
    "net = MyNet(\n",
    "    CrystalGraphConvNet,\n",
    "    module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "    module__nbr_fea_len = nbr_fea_len,\n",
    "    batch_size=batchsize, #214\n",
    "    module__classification=False,\n",
    "    lr=0.031661769560912136,\n",
    "    max_epochs= 200,\n",
    "    module__energy_mode=\"Harmonic\", #[\"Harmonic\", \"Morse\", \"LJ\"], Default = \"Harmonic\"\n",
    "    module__atom_fea_len=131, #46,\n",
    "    module__h_fea_len=251,\n",
    "    module__h_fea_len_dist=112,\n",
    "    module__h_fea_len_const=112,\n",
    "#     module__h_fea_len_D=(3,256),\n",
    "    module__n_conv=11, #8\n",
    "    module__n_h_dist=6,\n",
    "    module__n_h_const=6,\n",
    "    optimizer__weight_decay=0.000045399929762484854,\n",
    "#     module__n_h_D=(1,12),\n",
    "#     module__max_num_nbr=12, #9\n",
    "#     module__opt_step_size=(0.1,0.7), #0.3\n",
    "    module__min_opt_steps=30,\n",
    "    module__max_opt_steps=150,\n",
    "    module__momentum=0.8,\n",
    "    optimizer=AdamW,\n",
    "    iterator_train__pin_memory=True,\n",
    "    iterator_train__num_workers=0,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_train__collate_fn = collate_pool,\n",
    "    iterator_valid__pin_memory=True,\n",
    "    iterator_valid__num_workers=0,\n",
    "    iterator_valid__collate_fn = collate_pool,\n",
    "    device=device,\n",
    "#     criterion=torch.nn.MSELoss,\n",
    "    criterion=torch.nn.L1Loss,\n",
    "    dataset=MergeDataset,\n",
    "    train_split = CVSplit(cv=train_test_splitter),\n",
    "    callbacks=[cp,LR_schedule , load_best_valid_loss],\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, energy_mode, h_fea_len, h_fea_len_const, h_fea_len_dist, max_opt_steps, min_opt_steps, momentum, n_conv, n_h_const, n_h_dist, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer because the following parameters were re-set: weight_decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junwoony/miniconda3/envs/schnet2/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([287, 3]) torch.Size([287, 3])\n",
      "torch.Size([264, 3]) torch.Size([264, 3])\n",
      "torch.Size([275, 3]) torch.Size([275, 3])\n",
      "torch.Size([251, 3]) torch.Size([251, 3])\n",
      "torch.Size([300, 3]) torch.Size([300, 3])\n",
      "torch.Size([298, 3]) torch.Size([298, 3])\n",
      "torch.Size([288, 3]) torch.Size([288, 3])\n",
      "torch.Size([262, 3]) torch.Size([262, 3])\n",
      "torch.Size([207, 3]) torch.Size([207, 3])\n",
      "torch.Size([270, 3]) torch.Size([270, 3])\n",
      "torch.Size([280, 3]) torch.Size([280, 3])\n",
      "torch.Size([269, 3]) torch.Size([269, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.fit(SDT_training, target_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
